# Ollamaì™€ vLLM ê°€ì´ë“œ

## ëª©ì°¨
- [ê°œìš”](#ê°œìš”)
- [Ollama](#ollama)
  - [Ollamaë€?](#ollamaë€)
  - [ì„¤ì¹˜](#ollama-ì„¤ì¹˜)
  - [ê¸°ë³¸ ì‚¬ìš©ë²•](#ollama-ê¸°ë³¸-ì‚¬ìš©ë²•)
  - [ëª¨ë¸ ê´€ë¦¬](#ëª¨ë¸-ê´€ë¦¬)
  - [API ì‚¬ìš©](#ollama-api-ì‚¬ìš©)
  - [ì»¤ìŠ¤í…€ ëª¨ë¸ ìƒì„±](#ì»¤ìŠ¤í…€-ëª¨ë¸-ìƒì„±)
- [vLLM](#vllm)
  - [vLLMì´ë€?](#vllmì´ë€)
  - [ì„¤ì¹˜](#vllm-ì„¤ì¹˜)
  - [ê¸°ë³¸ ì‚¬ìš©ë²•](#vllm-ê¸°ë³¸-ì‚¬ìš©ë²•)
  - [OpenAI í˜¸í™˜ API ì„œë²„](#openai-í˜¸í™˜-api-ì„œë²„)
  - [ì„±ëŠ¥ ìµœì í™”](#ì„±ëŠ¥-ìµœì í™”)
- [ë¹„êµ ë° ì„ íƒ ê°€ì´ë“œ](#ë¹„êµ-ë°-ì„ íƒ-ê°€ì´ë“œ)
- [í™œìš© ì‚¬ë¡€](#í™œìš©-ì‚¬ë¡€)

---

## ê°œìš”

ì´ ê°€ì´ë“œëŠ” ë¡œì»¬ í™˜ê²½ì—ì„œ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì„ ì‹¤í–‰í•˜ê¸° ìœ„í•œ ë‘ ê°€ì§€ ì£¼ìš” ë„êµ¬ì¸ **Ollama**ì™€ **vLLM**ì— ëŒ€í•œ ìƒì„¸í•œ ì„¤ëª…ì„ ì œê³µí•©ë‹ˆë‹¤.

- **Ollama**: ì‚¬ìš©ì ì¹œí™”ì ì¸ ë¡œì»¬ LLM ì‹¤í–‰ ë„êµ¬
- **vLLM**: ê³ ì„±ëŠ¥ LLM ì¶”ë¡  ë° ì„œë¹™ ì—”ì§„

---

## Ollama

### Ollamaë€?

OllamaëŠ” ë¡œì»¬ í™˜ê²½ì—ì„œ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì„ ì‰½ê²Œ ì‹¤í–‰í•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„ëœ ì˜¤í”ˆì†ŒìŠ¤ ë„êµ¬ì…ë‹ˆë‹¤. Dockerì™€ ìœ ì‚¬í•œ ë°©ì‹ìœ¼ë¡œ ëª¨ë¸ì„ ê´€ë¦¬í•˜ë©°, ê°„ë‹¨í•œ ëª…ë ¹ì–´ë¡œ ë‹¤ì–‘í•œ LLMì„ ë‹¤ìš´ë¡œë“œí•˜ê³  ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

**ì£¼ìš” íŠ¹ì§•:**
- ğŸš€ ê°„í¸í•œ ì„¤ì¹˜ ë° ì‚¬ìš©
- ğŸ“¦ ë‹¤ì–‘í•œ ì‚¬ì „ í•™ìŠµ ëª¨ë¸ ì§€ì› (Llama 3, Mistral, Gemma ë“±)
- ğŸ”§ ì»¤ìŠ¤í…€ ëª¨ë¸ ìƒì„± ê°€ëŠ¥
- ğŸŒ REST API ì œê³µ
- ğŸ’» CPU ë° GPU ëª¨ë‘ ì§€ì›

### Ollama ì„¤ì¹˜

#### Windows

```bash
# Ollama ë‹¤ìš´ë¡œë“œ ë° ì„¤ì¹˜
# https://ollama.ai/download ì—ì„œ ì„¤ì¹˜ í”„ë¡œê·¸ë¨ ë‹¤ìš´ë¡œë“œ
```

#### macOS

```bash
# Homebrewë¥¼ ì‚¬ìš©í•œ ì„¤ì¹˜
brew install ollama

# ë˜ëŠ” ê³µì‹ ì›¹ì‚¬ì´íŠ¸ì—ì„œ ë‹¤ìš´ë¡œë“œ
# https://ollama.ai/download
```

#### Linux

```bash
# ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
curl -fsSL https://ollama.ai/install.sh | sh

# ì„œë¹„ìŠ¤ ì‹œì‘
sudo systemctl start ollama
sudo systemctl enable ollama
```

#### ì„¤ì¹˜ í™•ì¸

```bash
ollama --version
```

### Ollama ê¸°ë³¸ ì‚¬ìš©ë²•

#### 1. ëª¨ë¸ ì‹¤í–‰

```bash
# Llama 3 ëª¨ë¸ ì‹¤í–‰ (ìë™ ë‹¤ìš´ë¡œë“œ)
ollama run llama3

# ëŒ€í™”í˜• ëª¨ë“œ ì‹œì‘
>>> Hello, how are you?
```

#### 2. ì¼íšŒì„± í”„ë¡¬í”„íŠ¸ ì‹¤í–‰

```bash
# í•œ ë²ˆë§Œ ì§ˆë¬¸í•˜ê³  ì¢…ë£Œ
ollama run llama3 "Explain quantum computing in simple terms"
```

#### 3. ë‹¤ë¥¸ ëª¨ë¸ ì‚¬ìš©

```bash
# Mistral ëª¨ë¸
ollama run mistral

# Gemma ëª¨ë¸
ollama run gemma:7b

# CodeLlama (ì½”ë“œ ìƒì„±ì— íŠ¹í™”)
ollama run codellama
```

### ëª¨ë¸ ê´€ë¦¬

#### ëª¨ë¸ ëª©ë¡ í™•ì¸

```bash
# ë‹¤ìš´ë¡œë“œëœ ëª¨ë¸ ëª©ë¡
ollama list
```

#### ëª¨ë¸ ë‹¤ìš´ë¡œë“œ

```bash
# ëª¨ë¸ë§Œ ë‹¤ìš´ë¡œë“œ (ì‹¤í–‰í•˜ì§€ ì•ŠìŒ)
ollama pull llama3

# íŠ¹ì • í¬ê¸°ì˜ ëª¨ë¸
ollama pull llama3:70b
```

#### ëª¨ë¸ ì‚­ì œ

```bash
# ëª¨ë¸ ì œê±°
ollama rm llama3
```

#### ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ê²€ìƒ‰

```bash
# ëª¨ë¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ í™•ì¸
# https://ollama.ai/library ë°©ë¬¸
```

**ì¸ê¸° ëª¨ë¸:**
- `llama3` - Metaì˜ Llama 3 ëª¨ë¸
- `mistral` - Mistral AIì˜ ê³ ì„±ëŠ¥ ëª¨ë¸
- `gemma` - Googleì˜ ê²½ëŸ‰ ëª¨ë¸
- `codellama` - ì½”ë“œ ìƒì„± íŠ¹í™”
- `phi` - Microsoftì˜ ì†Œí˜• ëª¨ë¸
- `neural-chat` - ëŒ€í™” ìµœì í™” ëª¨ë¸

### Ollama API ì‚¬ìš©

OllamaëŠ” REST APIë¥¼ ì œê³µí•˜ì—¬ í”„ë¡œê·¸ë˜ë° ë°©ì‹ìœ¼ë¡œ ëª¨ë¸ê³¼ ìƒí˜¸ì‘ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

#### Python ì˜ˆì œ

```python
import requests
import json

# ê¸°ë³¸ ìƒì„± ìš”ì²­
def generate_text(prompt, model="llama3"):
    url = "http://localhost:11434/api/generate"
    data = {
        "model": model,
        "prompt": prompt,
        "stream": False
    }

    response = requests.post(url, json=data)
    return response.json()["response"]

# ì‚¬ìš© ì˜ˆ
result = generate_text("Pythonì—ì„œ ë¦¬ìŠ¤íŠ¸ ì»´í”„ë¦¬í—¨ì…˜ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”")
print(result)
```

#### ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ

```python
import requests
import json

def generate_streaming(prompt, model="llama3"):
    url = "http://localhost:11434/api/generate"
    data = {
        "model": model,
        "prompt": prompt,
        "stream": True
    }

    response = requests.post(url, json=data, stream=True)

    for line in response.iter_lines():
        if line:
            chunk = json.loads(line)
            if not chunk.get("done"):
                print(chunk.get("response"), end="", flush=True)

generate_streaming("ì¸ê³µì§€ëŠ¥ì˜ ë¯¸ë˜ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”")
```

#### ì±„íŒ… API

```python
def chat(messages, model="llama3"):
    url = "http://localhost:11434/api/chat"
    data = {
        "model": model,
        "messages": messages,
        "stream": False
    }

    response = requests.post(url, json=data)
    return response.json()["message"]["content"]

# ëŒ€í™”í˜• ì‚¬ìš©
messages = [
    {"role": "user", "content": "Pythonì´ë€ ë¬´ì—‡ì¸ê°€ìš”?"},
]

response = chat(messages)
print(response)

# ëŒ€í™” ì´ì–´ê°€ê¸°
messages.append({"role": "assistant", "content": response})
messages.append({"role": "user", "content": "ì£¼ìš” íŠ¹ì§•ì„ ì•Œë ¤ì£¼ì„¸ìš”"})

response = chat(messages)
print(response)
```

#### JavaScript/Node.js ì˜ˆì œ

```javascript
const axios = require('axios');

async function generateText(prompt, model = 'llama3') {
    const url = 'http://localhost:11434/api/generate';
    const response = await axios.post(url, {
        model: model,
        prompt: prompt,
        stream: false
    });

    return response.data.response;
}

// ì‚¬ìš©
generateText('JavaScriptì˜ async/awaitë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”')
    .then(result => console.log(result));
```

### ì»¤ìŠ¤í…€ ëª¨ë¸ ìƒì„±

OllamaëŠ” `Modelfile`ì„ ì‚¬ìš©í•˜ì—¬ ì»¤ìŠ¤í…€ ëª¨ë¸ì„ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

#### Modelfile ìƒì„±

```dockerfile
# Modelfile
FROM llama3

# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„¤ì •
SYSTEM """
ë‹¹ì‹ ì€ í•œêµ­ì–´ë¥¼ ìœ ì°½í•˜ê²Œ êµ¬ì‚¬í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
í•­ìƒ ì •ì¤‘í•˜ê³  ì¹œì ˆí•˜ê²Œ ë‹µë³€í•˜ë©°, ì „ë¬¸ì ì¸ ì§€ì‹ì„ ì œê³µí•©ë‹ˆë‹¤.
"""

# íŒŒë¼ë¯¸í„° ì„¤ì •
PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER top_k 40

# ë°˜ë³µ íŒ¨ë„í‹°
PARAMETER repeat_penalty 1.1
```

#### ëª¨ë¸ ìƒì„± ë° ì‹¤í–‰

```bash
# Modelfileë¡œë¶€í„° ëª¨ë¸ ìƒì„±
ollama create my-korean-assistant -f ./Modelfile

# ì»¤ìŠ¤í…€ ëª¨ë¸ ì‹¤í–‰
ollama run my-korean-assistant
```

#### ê³ ê¸‰ Modelfile ì˜ˆì œ (ì½”ë“œ ì–´ì‹œìŠ¤í„´íŠ¸)

```dockerfile
FROM codellama

SYSTEM """
You are an expert software engineer specializing in Python, JavaScript, and Go.
Provide clean, efficient, and well-documented code.
Always include error handling and follow best practices.
"""

PARAMETER temperature 0.3
PARAMETER num_ctx 4096

# í…œí”Œë¦¿ ì •ì˜
TEMPLATE """
{{ if .System }}<|system|>
{{ .System }}<|end|>
{{ end }}{{ if .Prompt }}<|user|>
{{ .Prompt }}<|end|>
<|assistant|>
{{ end }}
"""
```

### Ollama ì„¤ì • ë° ìµœì í™”

#### í™˜ê²½ ë³€ìˆ˜

```bash
# GPU ë©”ëª¨ë¦¬ ì œí•œ (GB)
export OLLAMA_GPU_MEMORY=8

# ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° í¬ê¸°
export OLLAMA_NUM_PARALLEL=4

# ì„œë²„ í˜¸ìŠ¤íŠ¸ ë° í¬íŠ¸
export OLLAMA_HOST=0.0.0.0:11434
```

#### ëª¨ë¸ ì‹¤í–‰ ì‹œ íŒŒë¼ë¯¸í„° ì¡°ì •

```bash
ollama run llama3 --verbose \
  --temperature 0.8 \
  --top-p 0.9 \
  --repeat-penalty 1.2
```

---

## vLLM

### vLLMì´ë€?

vLLM(Very Large Language Model)ì€ UC Berkeleyì˜ LMSYS ì—°êµ¬ì‹¤ì—ì„œ ê°œë°œí•œ ê³ ì„±ëŠ¥ LLM ì¶”ë¡  ë° ì„œë¹™ ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤. **PagedAttention** ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ì—¬ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ê·¹ëŒ€í™”í•˜ê³ , ì²˜ë¦¬ëŸ‰ì„ í¬ê²Œ í–¥ìƒì‹œí‚µë‹ˆë‹¤.

**ì£¼ìš” íŠ¹ì§•:**
- âš¡ ìµœëŒ€ 24ë°° ë¹ ë¥¸ ì²˜ë¦¬ëŸ‰
- ğŸ’¾ PagedAttentionìœ¼ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± í–¥ìƒ
- ğŸ”„ ì—°ì† ë°°ì¹­ìœ¼ë¡œ ì—¬ëŸ¬ ìš”ì²­ ë™ì‹œ ì²˜ë¦¬
- ğŸŒ OpenAI í˜¸í™˜ API ì„œë²„
- ğŸ¯ í…ì„œ ë³‘ë ¬í™” ì§€ì› (ë‹¤ì¤‘ GPU)
- ğŸ“Š CUDA, ROCm ì§€ì›

### vLLM ì„¤ì¹˜

#### ìš”êµ¬ì‚¬í•­

- Python 3.8 ì´ìƒ
- CUDA 11.8 ì´ìƒ (GPU ì‚¬ìš© ì‹œ)
- Linux (ê¶Œì¥) ë˜ëŠ” WSL2 (Windows)

#### pipë¥¼ í†µí•œ ì„¤ì¹˜

```bash
# CUDA 12.1
pip install vllm

# ë˜ëŠ” íŠ¹ì • CUDA ë²„ì „
pip install vllm-cuda11  # CUDA 11.xìš©
```

#### ì†ŒìŠ¤ë¡œë¶€í„° ì„¤ì¹˜

```bash
git clone https://github.com/vllm-project/vllm.git
cd vllm
pip install -e .
```

#### Docker ì‚¬ìš©

```bash
# vLLM Docker ì´ë¯¸ì§€ ì‹¤í–‰
docker run --gpus all \
    -v ~/.cache/huggingface:/root/.cache/huggingface \
    -p 8000:8000 \
    --ipc=host \
    vllm/vllm-openai:latest \
    --model facebook/opt-125m
```

### vLLM ê¸°ë³¸ ì‚¬ìš©ë²•

#### Pythonì—ì„œ ì§ì ‘ ì‚¬ìš©

```python
from vllm import LLM, SamplingParams

# ëª¨ë¸ ë¡œë“œ
llm = LLM(model="meta-llama/Llama-3-8b-hf")

# ìƒ˜í”Œë§ íŒŒë¼ë¯¸í„° ì„¤ì •
sampling_params = SamplingParams(
    temperature=0.8,
    top_p=0.95,
    max_tokens=100
)

# í”„ë¡¬í”„íŠ¸
prompts = [
    "Pythonì—ì„œ ë°ì½”ë ˆì´í„°ë€ ë¬´ì—‡ì¸ê°€ìš”?",
    "ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ì˜ ì°¨ì´ëŠ”?",
]

# ìƒì„±
outputs = llm.generate(prompts, sampling_params)

# ê²°ê³¼ ì¶œë ¥
for output in outputs:
    prompt = output.prompt
    generated_text = output.outputs[0].text
    print(f"Prompt: {prompt}")
    print(f"Generated: {generated_text}")
    print("-" * 50)
```

#### ë°°ì¹˜ ì²˜ë¦¬

```python
from vllm import LLM, SamplingParams

# ëª¨ë¸ ì´ˆê¸°í™”
llm = LLM(
    model="mistralai/Mistral-7B-v0.1",
    tensor_parallel_size=2,  # 2ê°œ GPU ì‚¬ìš©
    gpu_memory_utilization=0.9
)

# ëŒ€ëŸ‰ì˜ í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬
prompts = [f"Tell me about topic {i}" for i in range(100)]

sampling_params = SamplingParams(temperature=0.7, max_tokens=50)

# íš¨ìœ¨ì ì¸ ë°°ì¹˜ ì²˜ë¦¬
outputs = llm.generate(prompts, sampling_params)
```

### OpenAI í˜¸í™˜ API ì„œë²„

vLLMì€ OpenAI APIì™€ í˜¸í™˜ë˜ëŠ” ì„œë²„ë¥¼ ì œê³µí•˜ì—¬ ê¸°ì¡´ OpenAI í´ë¼ì´ì–¸íŠ¸ ì½”ë“œë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

#### ì„œë²„ ì‹œì‘

```bash
# ê¸°ë³¸ ì‹¤í–‰
python -m vllm.entrypoints.openai.api_server \
    --model meta-llama/Llama-3-8b-hf \
    --port 8000

# GPU ìµœì í™” ì„¤ì •
python -m vllm.entrypoints.openai.api_server \
    --model meta-llama/Llama-3-8b-hf \
    --port 8000 \
    --tensor-parallel-size 2 \
    --gpu-memory-utilization 0.95 \
    --max-model-len 4096
```

#### OpenAI Python í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš©

```python
from openai import OpenAI

# vLLM ì„œë²„ì— ì—°ê²°
client = OpenAI(
    base_url="http://localhost:8000/v1",
    api_key="not-needed"  # vLLMì€ API í‚¤ê°€ í•„ìš” ì—†ìŒ
)

# ì±„íŒ… ì™„ì„±
response = client.chat.completions.create(
    model="meta-llama/Llama-3-8b-hf",
    messages=[
        {"role": "system", "content": "ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."},
        {"role": "user", "content": "Pythonì˜ ì¥ì ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”"}
    ],
    temperature=0.7,
    max_tokens=200
)

print(response.choices[0].message.content)
```

#### ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ

```python
from openai import OpenAI

client = OpenAI(
    base_url="http://localhost:8000/v1",
    api_key="not-needed"
)

# ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ
stream = client.chat.completions.create(
    model="meta-llama/Llama-3-8b-hf",
    messages=[
        {"role": "user", "content": "ì¸ê³µì§€ëŠ¥ì˜ ì—­ì‚¬ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”"}
    ],
    stream=True
)

for chunk in stream:
    if chunk.choices[0].delta.content is not None:
        print(chunk.choices[0].delta.content, end="", flush=True)
```

#### cURLì„ ì‚¬ìš©í•œ API í˜¸ì¶œ

```bash
# ì±„íŒ… ì™„ì„±
curl http://localhost:8000/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "meta-llama/Llama-3-8b-hf",
        "messages": [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": "What is machine learning?"}
        ],
        "temperature": 0.7,
        "max_tokens": 100
    }'

# í…ìŠ¤íŠ¸ ì™„ì„±
curl http://localhost:8000/v1/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "meta-llama/Llama-3-8b-hf",
        "prompt": "Once upon a time",
        "max_tokens": 50,
        "temperature": 0.8
    }'
```

### ì„±ëŠ¥ ìµœì í™”

#### 1. GPU ë©”ëª¨ë¦¬ í™œìš© ìµœì í™”

```python
from vllm import LLM

llm = LLM(
    model="meta-llama/Llama-3-8b-hf",
    gpu_memory_utilization=0.95,  # GPU ë©”ëª¨ë¦¬ 95% ì‚¬ìš©
    max_model_len=4096,  # ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´
    enforce_eager=False,  # CUDA ê·¸ë˜í”„ ì‚¬ìš©
)
```

#### 2. ë‹¤ì¤‘ GPU í…ì„œ ë³‘ë ¬í™”

```python
from vllm import LLM

# 4ê°œ GPUì— ëª¨ë¸ ë¶„ì‚°
llm = LLM(
    model="meta-llama/Llama-3-70b-hf",
    tensor_parallel_size=4,
    dtype="float16"
)
```

#### 3. ì–‘ìí™” ì‚¬ìš©

```python
from vllm import LLM

# AWQ ì–‘ìí™” ëª¨ë¸ ì‚¬ìš©
llm = LLM(
    model="TheBloke/Llama-2-7B-AWQ",
    quantization="awq",
    dtype="half"
)

# GPTQ ì–‘ìí™”
llm = LLM(
    model="TheBloke/Llama-2-7B-GPTQ",
    quantization="gptq"
)
```

#### 4. í”„ë¦¬í”½ìŠ¤ ìºì‹±

```python
from vllm import LLM, SamplingParams

llm = LLM(
    model="meta-llama/Llama-3-8b-hf",
    enable_prefix_caching=True  # ê³µí†µ í”„ë¦¬í”½ìŠ¤ ìºì‹±
)

# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ê³µìœ í•˜ëŠ” ì—¬ëŸ¬ ìš”ì²­ì— ìœ ë¦¬
system_prompt = "You are an expert Python programmer."
prompts = [
    system_prompt + "\n\nExplain decorators.",
    system_prompt + "\n\nExplain generators.",
    system_prompt + "\n\nExplain context managers."
]

outputs = llm.generate(prompts, SamplingParams(max_tokens=100))
```

#### 5. ì„œë²„ ì„±ëŠ¥ íŠœë‹

```bash
python -m vllm.entrypoints.openai.api_server \
    --model meta-llama/Llama-3-8b-hf \
    --tensor-parallel-size 2 \
    --gpu-memory-utilization 0.95 \
    --max-model-len 4096 \
    --max-num-seqs 256 \
    --disable-log-requests \
    --dtype float16
```

### ê³ ê¸‰ ê¸°ëŠ¥

#### LoRA ì–´ëŒ‘í„° ì‚¬ìš©

```python
from vllm import LLM, SamplingParams
from vllm.lora.request import LoRARequest

# ê¸°ë³¸ ëª¨ë¸ ë¡œë“œ
llm = LLM(
    model="meta-llama/Llama-3-8b-hf",
    enable_lora=True,
    max_lora_rank=64
)

# LoRA ì–´ëŒ‘í„°ì™€ í•¨ê»˜ ìƒì„±
outputs = llm.generate(
    "Translate to French: Hello, how are you?",
    SamplingParams(temperature=0.7, max_tokens=50),
    lora_request=LoRARequest("translation", 1, "/path/to/lora/adapter")
)
```

#### ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ (ë¹„ì „)

```python
from vllm import LLM, SamplingParams

# LLaVAì™€ ê°™ì€ ë¹„ì „-ì–¸ì–´ ëª¨ë¸
llm = LLM(model="llava-hf/llava-1.5-7b-hf")

# ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸
outputs = llm.generate({
    "prompt": "USER: <image>\nWhat is in this image?\nASSISTANT:",
    "multi_modal_data": {"image": "/path/to/image.jpg"}
})
```

---

## ë¹„êµ ë° ì„ íƒ ê°€ì´ë“œ

### Ollama vs vLLM ë¹„êµ

| íŠ¹ì§• | Ollama | vLLM |
|------|--------|------|
| **ì‚¬ìš© í¸ì˜ì„±** | â­â­â­â­â­ ë§¤ìš° ì‰¬ì›€ | â­â­â­ ì¤‘ê°„ |
| **ì„±ëŠ¥** | â­â­â­ ì¢‹ìŒ | â­â­â­â­â­ ë›°ì–´ë‚¨ |
| **ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±** | â­â­â­ ì¢‹ìŒ | â­â­â­â­â­ ë§¤ìš° ë†’ìŒ |
| **ì²˜ë¦¬ëŸ‰** | â­â­â­ ë‹¨ì¼ ìš”ì²­ì— ì í•© | â­â­â­â­â­ ëŒ€ëŸ‰ ìš”ì²­ ì²˜ë¦¬ |
| **ëª¨ë¸ ê´€ë¦¬** | â­â­â­â­â­ ë§¤ìš° ì‰¬ì›€ | â­â­â­ ìˆ˜ë™ ê´€ë¦¬ |
| **ì»¤ìŠ¤í„°ë§ˆì´ì§•** | â­â­â­â­ Modelfile ì§€ì› | â­â­â­â­â­ ê³ ê¸‰ ì˜µì…˜ í’ë¶€ |
| **ë‹¤ì¤‘ GPU ì§€ì›** | â­â­ ì œí•œì  | â­â­â­â­â­ í…ì„œ ë³‘ë ¬í™” |
| **API í˜¸í™˜ì„±** | REST API | OpenAI í˜¸í™˜ API |
| **í”Œë«í¼ ì§€ì›** | Windows, macOS, Linux | Linux (ê¶Œì¥), WSL2 |
| **í•™ìŠµ ê³¡ì„ ** | ë‚®ìŒ | ì¤‘ê°„-ë†’ìŒ |

### ì–¸ì œ Ollamaë¥¼ ì‚¬ìš©í• ê¹Œ?

âœ… **Ollamaë¥¼ ì„ íƒí•˜ì„¸ìš”:**
- ê°œì¸ ìš©ë„ ë˜ëŠ” ì†Œê·œëª¨ í”„ë¡œì íŠ¸
- ë¹ ë¥´ê³  ì‰¬ìš´ ì„¤ì •ì´ í•„ìš”í•  ë•Œ
- ëª…ë ¹ì¤„ì—ì„œ ê°„ë‹¨í•˜ê²Œ ëª¨ë¸ì„ í…ŒìŠ¤íŠ¸í•˜ê³  ì‹¶ì„ ë•Œ
- Docker ìŠ¤íƒ€ì¼ì˜ ëª¨ë¸ ê´€ë¦¬ë¥¼ ì„ í˜¸í•  ë•Œ
- macOS ë˜ëŠ” Windows í™˜ê²½
- ë™ì‹œ ìš”ì²­ì´ ë§ì§€ ì•Šì„ ë•Œ

### ì–¸ì œ vLLMì„ ì‚¬ìš©í• ê¹Œ?

âœ… **vLLMì„ ì„ íƒí•˜ì„¸ìš”:**
- í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œ ë†’ì€ ì²˜ë¦¬ëŸ‰ì´ í•„ìš”í•  ë•Œ
- ì—¬ëŸ¬ GPUë¥¼ í™œìš©í•œ ëŒ€í˜• ëª¨ë¸ ì‹¤í–‰
- ë™ì‹œì— ë§ì€ ìš”ì²­ì„ ì²˜ë¦¬í•´ì•¼ í•  ë•Œ
- ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì´ ì¤‘ìš”í•  ë•Œ
- OpenAI API í˜¸í™˜ì„±ì´ í•„ìš”í•  ë•Œ
- ìµœëŒ€ ì„±ëŠ¥ì´ í•„ìš”í•œ ì„œë¹„ìŠ¤ êµ¬ì¶•
- ë°°ì¹˜ ì¶”ë¡  ì‘ì—…

### í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼

ë‘ ë„êµ¬ë¥¼ í•¨ê»˜ ì‚¬ìš©í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤:
- **ê°œë°œ**: Ollamaë¡œ ë¹ ë¥´ê²Œ í”„ë¡œí† íƒ€ì… ê°œë°œ ë° í…ŒìŠ¤íŠ¸
- **í”„ë¡œë•ì…˜**: vLLMìœ¼ë¡œ ê³ ì„±ëŠ¥ ì„œë¹„ìŠ¤ ë°°í¬

---

## NVIDIA GPU ì„œë²„ í”„ë¡œë•ì…˜ ë°°í¬

### GPU ì„œë²„ í™˜ê²½ ì„¤ì •

#### 1. NVIDIA ë“œë¼ì´ë²„ ë° CUDA ì„¤ì¹˜

```bash
# NVIDIA ë“œë¼ì´ë²„ í™•ì¸
nvidia-smi

# CUDA ë²„ì „ í™•ì¸
nvcc --version

# NVIDIA ë“œë¼ì´ë²„ ì„¤ì¹˜ (Ubuntu/Debian)
sudo apt update
sudo apt install -y nvidia-driver-535

# CUDA Toolkit ì„¤ì¹˜
wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.1-1_all.deb
sudo dpkg -i cuda-keyring_1.1-1_all.deb
sudo apt update
sudo apt install -y cuda-toolkit-12-1

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
echo 'export PATH=/usr/local/cuda/bin:$PATH' >> ~/.bashrc
echo 'export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH' >> ~/.bashrc
source ~/.bashrc
```

#### 2. Docker ë° NVIDIA Container Toolkit ì„¤ì¹˜

```bash
# Docker ì„¤ì¹˜
curl -fsSL https://get.docker.com -o get-docker.sh
sudo sh get-docker.sh

# NVIDIA Container Toolkit ì„¤ì¹˜
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | \
    sudo tee /etc/apt/sources.list.d/nvidia-docker.list

sudo apt update
sudo apt install -y nvidia-container-toolkit

# Docker ì¬ì‹œì‘
sudo systemctl restart docker

# GPU ì ‘ê·¼ í…ŒìŠ¤íŠ¸
docker run --rm --gpus all nvidia/cuda:12.1.0-base-ubuntu22.04 nvidia-smi
```

### Ollama GPU ì„œë²„ êµ¬ì„±

#### 1. ì‹œìŠ¤í…œ ì„œë¹„ìŠ¤ë¡œ Ollama ì„¤ì •

```bash
# Ollama ì„¤ì¹˜
curl -fsSL https://ollama.ai/install.sh | sh

# GPU ë©”ëª¨ë¦¬ ì„¤ì • (í™˜ê²½ ë³€ìˆ˜)
sudo tee /etc/systemd/system/ollama.service.d/override.conf <<EOF
[Service]
Environment="OLLAMA_HOST=0.0.0.0:11434"
Environment="OLLAMA_ORIGINS=*"
Environment="OLLAMA_NUM_PARALLEL=4"
Environment="OLLAMA_MAX_LOADED_MODELS=2"
Environment="OLLAMA_GPU_OVERHEAD=0"
EOF

# ì„œë¹„ìŠ¤ ì¬ì‹œì‘
sudo systemctl daemon-reload
sudo systemctl restart ollama
sudo systemctl enable ollama

# ìƒíƒœ í™•ì¸
sudo systemctl status ollama
```

#### 2. Ollama Docker ë°°í¬

```bash
# docker-compose.yml ìƒì„±
cat > docker-compose.yml <<EOF
version: '3.8'

services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama-server
    restart: unless-stopped
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0:11434
      - OLLAMA_ORIGINS=*
      - OLLAMA_NUM_PARALLEL=4
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

volumes:
  ollama-data:
EOF

# ì»¨í…Œì´ë„ˆ ì‹œì‘
docker-compose up -d

# ë¡œê·¸ í™•ì¸
docker-compose logs -f ollama

# ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
docker exec -it ollama-server ollama pull llama3
docker exec -it ollama-server ollama pull mistral
```

#### 3. Nginx ë¦¬ë²„ìŠ¤ í”„ë¡ì‹œ ì„¤ì •

```bash
# Nginx ì„¤ì¹˜
sudo apt install -y nginx

# Ollama í”„ë¡ì‹œ ì„¤ì •
sudo tee /etc/nginx/sites-available/ollama <<EOF
upstream ollama_backend {
    server localhost:11434;
    keepalive 32;
}

server {
    listen 80;
    server_name ollama.yourdomain.com;

    # SSL ì„¤ì • (Let's Encrypt ì‚¬ìš© ì‹œ)
    # listen 443 ssl http2;
    # ssl_certificate /etc/letsencrypt/live/ollama.yourdomain.com/fullchain.pem;
    # ssl_certificate_key /etc/letsencrypt/live/ollama.yourdomain.com/privkey.pem;

    client_max_body_size 100M;

    location / {
        proxy_pass http://ollama_backend;
        proxy_http_version 1.1;
        proxy_set_header Connection "";
        proxy_set_header Host \$host;
        proxy_set_header X-Real-IP \$remote_addr;
        proxy_set_header X-Forwarded-For \$proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto \$scheme;

        # íƒ€ì„ì•„ì›ƒ ì„¤ì • (ê¸´ ì‘ë‹µ ëŒ€ë¹„)
        proxy_connect_timeout 300s;
        proxy_send_timeout 300s;
        proxy_read_timeout 300s;
    }
}
EOF

# ì‹¬ë³¼ë¦­ ë§í¬ ìƒì„±
sudo ln -s /etc/nginx/sites-available/ollama /etc/nginx/sites-enabled/

# Nginx ì¬ì‹œì‘
sudo nginx -t
sudo systemctl restart nginx
```

### vLLM GPU ì„œë²„ êµ¬ì„±

#### 1. Python í™˜ê²½ ì„¤ì •

```bash
# Python 3.10+ ì„¤ì¹˜
sudo apt install -y python3.10 python3.10-venv python3-pip

# ê°€ìƒ í™˜ê²½ ìƒì„±
python3.10 -m venv vllm-env
source vllm-env/bin/activate

# vLLM ì„¤ì¹˜
pip install vllm
pip install ray  # ë¶„ì‚° ì²˜ë¦¬ìš©
```

#### 2. vLLM ì„œë¹„ìŠ¤ ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±

```bash
# vLLM ì„œë¹„ìŠ¤ ìŠ¤í¬ë¦½íŠ¸
cat > /opt/vllm/start_vllm.sh <<'EOF'
#!/bin/bash

MODEL_NAME="meta-llama/Llama-3-8b-hf"
TENSOR_PARALLEL_SIZE=1  # GPU ìˆ˜ì— ë§ê²Œ ì¡°ì •
PORT=8000

python -m vllm.entrypoints.openai.api_server \
    --model $MODEL_NAME \
    --host 0.0.0.0 \
    --port $PORT \
    --tensor-parallel-size $TENSOR_PARALLEL_SIZE \
    --gpu-memory-utilization 0.95 \
    --max-model-len 4096 \
    --max-num-seqs 256 \
    --dtype auto \
    --trust-remote-code
EOF

chmod +x /opt/vllm/start_vllm.sh
```

#### 3. Systemd ì„œë¹„ìŠ¤ ì„¤ì •

```bash
# vLLM systemd ì„œë¹„ìŠ¤
sudo tee /etc/systemd/system/vllm.service <<EOF
[Unit]
Description=vLLM OpenAI Compatible API Server
After=network.target

[Service]
Type=simple
User=ubuntu
WorkingDirectory=/opt/vllm
Environment="PATH=/home/ubuntu/vllm-env/bin:/usr/local/cuda/bin:\$PATH"
Environment="LD_LIBRARY_PATH=/usr/local/cuda/lib64:\$LD_LIBRARY_PATH"
Environment="CUDA_VISIBLE_DEVICES=0"
ExecStart=/opt/vllm/start_vllm.sh
Restart=on-failure
RestartSec=10
StandardOutput=journal
StandardError=journal

[Install]
WantedBy=multi-user.target
EOF

# ì„œë¹„ìŠ¤ ì‹œì‘
sudo systemctl daemon-reload
sudo systemctl start vllm
sudo systemctl enable vllm

# ìƒíƒœ í™•ì¸
sudo systemctl status vllm
journalctl -u vllm -f
```

#### 4. vLLM Docker ë°°í¬ (ë©€í‹° GPU)

```yaml
# docker-compose-vllm.yml
version: '3.8'

services:
  vllm-api:
    image: vllm/vllm-openai:latest
    container_name: vllm-server
    restart: unless-stopped
    ports:
      - "8000:8000"
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}  # í•„ìš” ì‹œ
    command: >
      --model meta-llama/Llama-3-8b-hf
      --host 0.0.0.0
      --port 8000
      --tensor-parallel-size 2
      --gpu-memory-utilization 0.95
      --max-model-len 4096
      --dtype auto
    shm_size: '8gb'
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 2  # ì‚¬ìš©í•  GPU ìˆ˜
              capabilities: [gpu]
```

```bash
# ì‹¤í–‰
docker-compose -f docker-compose-vllm.yml up -d

# ë¡œê·¸ í™•ì¸
docker logs -f vllm-server
```

#### 5. Rayë¥¼ ì‚¬ìš©í•œ ë¶„ì‚° vLLM í´ëŸ¬ìŠ¤í„°

```python
# distributed_vllm.py
from vllm import LLM, SamplingParams
import ray

# Ray ì´ˆê¸°í™”
ray.init()

# ì—¬ëŸ¬ ë…¸ë“œì— ê±¸ì³ ëª¨ë¸ ë°°í¬
@ray.remote(num_gpus=1)
class LLMWorker:
    def __init__(self, model_name):
        self.llm = LLM(
            model=model_name,
            tensor_parallel_size=1,
            gpu_memory_utilization=0.95
        )

    def generate(self, prompts, params):
        return self.llm.generate(prompts, params)

# ì—¬ëŸ¬ ì›Œì»¤ ìƒì„±
workers = [
    LLMWorker.remote("meta-llama/Llama-3-8b-hf")
    for _ in range(4)  # 4ê°œ GPU
]

# ë¶€í•˜ ë¶„ì‚°
def distributed_generate(prompts, params):
    chunk_size = len(prompts) // len(workers)
    chunks = [
        prompts[i:i+chunk_size]
        for i in range(0, len(prompts), chunk_size)
    ]

    futures = [
        worker.generate.remote(chunk, params)
        for worker, chunk in zip(workers, chunks)
    ]

    results = ray.get(futures)
    return [item for sublist in results for item in sublist]
```

### í”„ë¡œë•ì…˜ ì• í”Œë¦¬ì¼€ì´ì…˜ êµ¬ì¶•

#### 1. FastAPI ê¸°ë°˜ LLM ì„œë¹„ìŠ¤

```python
# app.py
from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Optional
import httpx
import asyncio
from datetime import datetime
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(title="LLM Service API", version="1.0.0")

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ë°±ì—”ë“œ ì„¤ì •
OLLAMA_URL = "http://localhost:11434"
VLLM_URL = "http://localhost:8000/v1"

class GenerateRequest(BaseModel):
    prompt: str
    model: str = "llama3"
    temperature: float = 0.7
    max_tokens: int = 500
    backend: str = "ollama"  # "ollama" or "vllm"

class ChatMessage(BaseModel):
    role: str
    content: str

class ChatRequest(BaseModel):
    messages: List[ChatMessage]
    model: str = "llama3"
    temperature: float = 0.7
    max_tokens: int = 500
    stream: bool = False
    backend: str = "vllm"

# Ollama ìƒì„±
async def generate_ollama(prompt: str, model: str, temperature: float, max_tokens: int):
    async with httpx.AsyncClient(timeout=300.0) as client:
        try:
            response = await client.post(
                f"{OLLAMA_URL}/api/generate",
                json={
                    "model": model,
                    "prompt": prompt,
                    "temperature": temperature,
                    "options": {
                        "num_predict": max_tokens
                    },
                    "stream": False
                }
            )
            response.raise_for_status()
            return response.json()
        except Exception as e:
            logger.error(f"Ollama error: {e}")
            raise HTTPException(status_code=500, detail=str(e))

# vLLM ìƒì„±
async def generate_vllm(prompt: str, model: str, temperature: float, max_tokens: int):
    async with httpx.AsyncClient(timeout=300.0) as client:
        try:
            response = await client.post(
                f"{VLLM_URL}/completions",
                json={
                    "model": model,
                    "prompt": prompt,
                    "temperature": temperature,
                    "max_tokens": max_tokens
                },
                headers={"Content-Type": "application/json"}
            )
            response.raise_for_status()
            return response.json()
        except Exception as e:
            logger.error(f"vLLM error: {e}")
            raise HTTPException(status_code=500, detail=str(e))

# vLLM ì±„íŒ…
async def chat_vllm(messages: List[dict], model: str, temperature: float, max_tokens: int):
    async with httpx.AsyncClient(timeout=300.0) as client:
        try:
            response = await client.post(
                f"{VLLM_URL}/chat/completions",
                json={
                    "model": model,
                    "messages": messages,
                    "temperature": temperature,
                    "max_tokens": max_tokens
                }
            )
            response.raise_for_status()
            return response.json()
        except Exception as e:
            logger.error(f"vLLM chat error: {e}")
            raise HTTPException(status_code=500, detail=str(e))

@app.get("/")
async def root():
    return {
        "message": "LLM Service API",
        "version": "1.0.0",
        "backends": ["ollama", "vllm"]
    }

@app.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat()
    }

@app.post("/generate")
async def generate(request: GenerateRequest):
    """í…ìŠ¤íŠ¸ ìƒì„± ì—”ë“œí¬ì¸íŠ¸"""
    logger.info(f"Generate request - Backend: {request.backend}, Model: {request.model}")

    if request.backend == "ollama":
        result = await generate_ollama(
            request.prompt,
            request.model,
            request.temperature,
            request.max_tokens
        )
        return {
            "text": result.get("response", ""),
            "model": request.model,
            "backend": "ollama"
        }
    elif request.backend == "vllm":
        result = await generate_vllm(
            request.prompt,
            request.model,
            request.temperature,
            request.max_tokens
        )
        return {
            "text": result["choices"][0]["text"],
            "model": request.model,
            "backend": "vllm"
        }
    else:
        raise HTTPException(status_code=400, detail="Invalid backend")

@app.post("/chat")
async def chat(request: ChatRequest):
    """ì±„íŒ… ì—”ë“œí¬ì¸íŠ¸"""
    logger.info(f"Chat request - Backend: {request.backend}, Model: {request.model}")

    messages = [{"role": msg.role, "content": msg.content} for msg in request.messages]

    result = await chat_vllm(
        messages,
        request.model,
        request.temperature,
        request.max_tokens
    )

    return {
        "message": result["choices"][0]["message"]["content"],
        "model": request.model,
        "backend": "vllm"
    }

@app.post("/batch")
async def batch_generate(prompts: List[str], model: str = "llama3", backend: str = "vllm"):
    """ë°°ì¹˜ ì²˜ë¦¬ ì—”ë“œí¬ì¸íŠ¸"""
    logger.info(f"Batch request - Count: {len(prompts)}, Backend: {backend}")

    tasks = [
        generate_vllm(prompt, model, 0.7, 500)
        for prompt in prompts
    ]

    results = await asyncio.gather(*tasks, return_exceptions=True)

    return {
        "results": [
            r["choices"][0]["text"] if not isinstance(r, Exception) else str(r)
            for r in results
        ],
        "count": len(prompts)
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8080, workers=4)
```

#### 2. ì• í”Œë¦¬ì¼€ì´ì…˜ ë°°í¬ ìŠ¤í¬ë¦½íŠ¸

```bash
# deploy.sh
#!/bin/bash

# ì˜ì¡´ì„± ì„¤ì¹˜
pip install fastapi uvicorn[standard] httpx pydantic

# ì„œë¹„ìŠ¤ íŒŒì¼ ìƒì„±
sudo tee /etc/systemd/system/llm-api.service <<EOF
[Unit]
Description=LLM API Service
After=network.target vllm.service ollama.service

[Service]
Type=simple
User=ubuntu
WorkingDirectory=/opt/llm-service
Environment="PATH=/home/ubuntu/vllm-env/bin:\$PATH"
ExecStart=/home/ubuntu/vllm-env/bin/uvicorn app:app --host 0.0.0.0 --port 8080 --workers 4
Restart=always
RestartSec=10

[Install]
WantedBy=multi-user.target
EOF

# ì„œë¹„ìŠ¤ ì‹œì‘
sudo systemctl daemon-reload
sudo systemctl start llm-api
sudo systemctl enable llm-api
```

#### 3. Docker Compose ì „ì²´ ìŠ¤íƒ

```yaml
# docker-compose-full.yml
version: '3.8'

services:
  # vLLM ì„œë²„
  vllm:
    image: vllm/vllm-openai:latest
    container_name: vllm-server
    restart: unless-stopped
    ports:
      - "8000:8000"
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
    command: >
      --model meta-llama/Llama-3-8b-hf
      --host 0.0.0.0
      --port 8000
      --tensor-parallel-size 1
      --gpu-memory-utilization 0.9
    shm_size: '8gb'
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0']
              capabilities: [gpu]

  # Ollama ì„œë²„
  ollama:
    image: ollama/ollama:latest
    container_name: ollama-server
    restart: unless-stopped
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0:11434
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['1']
              capabilities: [gpu]

  # FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜
  llm-api:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: llm-api
    restart: unless-stopped
    ports:
      - "8080:8080"
    depends_on:
      - vllm
      - ollama
    environment:
      - OLLAMA_URL=http://ollama:11434
      - VLLM_URL=http://vllm:8000/v1

  # Nginx ë¦¬ë²„ìŠ¤ í”„ë¡ì‹œ
  nginx:
    image: nginx:alpine
    container_name: nginx-proxy
    restart: unless-stopped
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
      - ./ssl:/etc/nginx/ssl:ro
    depends_on:
      - llm-api

  # Redis (ìºì‹±ìš©)
  redis:
    image: redis:alpine
    container_name: redis-cache
    restart: unless-stopped
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data

  # Prometheus (ëª¨ë‹ˆí„°ë§)
  prometheus:
    image: prom/prometheus
    container_name: prometheus
    restart: unless-stopped
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus-data:/prometheus

  # Grafana (ëŒ€ì‹œë³´ë“œ)
  grafana:
    image: grafana/grafana
    container_name: grafana
    restart: unless-stopped
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
    volumes:
      - grafana-data:/var/lib/grafana

volumes:
  ollama-data:
  redis-data:
  prometheus-data:
  grafana-data:
```

#### 4. Dockerfile

```dockerfile
# Dockerfile
FROM python:3.10-slim

WORKDIR /app

# ì˜ì¡´ì„± ì„¤ì¹˜
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# ì• í”Œë¦¬ì¼€ì´ì…˜ ë³µì‚¬
COPY app.py .

# í¬íŠ¸ ë…¸ì¶œ
EXPOSE 8080

# ì‹¤í–‰
CMD ["uvicorn", "app:app", "--host", "0.0.0.0", "--port", "8080", "--workers", "4"]
```

```txt
# requirements.txt
fastapi==0.109.0
uvicorn[standard]==0.27.0
httpx==0.26.0
pydantic==2.5.0
redis==5.0.1
prometheus-client==0.19.0
```

### ëª¨ë‹ˆí„°ë§ ë° ê´€ë¦¬

#### 1. GPU ëª¨ë‹ˆí„°ë§ ìŠ¤í¬ë¦½íŠ¸

```python
# gpu_monitor.py
import subprocess
import time
from prometheus_client import start_http_server, Gauge
import re

# Prometheus ë©”íŠ¸ë¦­ ì •ì˜
gpu_utilization = Gauge('gpu_utilization_percent', 'GPU utilization', ['gpu_id'])
gpu_memory_used = Gauge('gpu_memory_used_mb', 'GPU memory used', ['gpu_id'])
gpu_memory_total = Gauge('gpu_memory_total_mb', 'GPU memory total', ['gpu_id'])
gpu_temperature = Gauge('gpu_temperature_celsius', 'GPU temperature', ['gpu_id'])

def get_gpu_stats():
    """nvidia-smië¡œ GPU í†µê³„ ìˆ˜ì§‘"""
    result = subprocess.run(
        ['nvidia-smi', '--query-gpu=index,utilization.gpu,memory.used,memory.total,temperature.gpu',
         '--format=csv,noheader,nounits'],
        capture_output=True,
        text=True
    )

    for line in result.stdout.strip().split('\n'):
        gpu_id, util, mem_used, mem_total, temp = line.split(', ')

        gpu_utilization.labels(gpu_id=gpu_id).set(float(util))
        gpu_memory_used.labels(gpu_id=gpu_id).set(float(mem_used))
        gpu_memory_total.labels(gpu_id=gpu_id).set(float(mem_total))
        gpu_temperature.labels(gpu_id=gpu_id).set(float(temp))

if __name__ == '__main__':
    # Prometheus ì„œë²„ ì‹œì‘ (í¬íŠ¸ 8001)
    start_http_server(8001)

    # 5ì´ˆë§ˆë‹¤ GPU í†µê³„ ìˆ˜ì§‘
    while True:
        get_gpu_stats()
        time.sleep(5)
```

#### 2. Prometheus ì„¤ì •

```yaml
# prometheus.yml
global:
  scrape_interval: 15s

scrape_configs:
  - job_name: 'gpu_metrics'
    static_configs:
      - targets: ['localhost:8001']

  - job_name: 'llm_api'
    static_configs:
      - targets: ['localhost:8080']

  - job_name: 'vllm'
    static_configs:
      - targets: ['localhost:8000']
```

#### 3. ë¡œê·¸ ìˆ˜ì§‘ ë° ë¶„ì„

```python
# logging_config.py
import logging
from logging.handlers import RotatingFileHandler
import json
from datetime import datetime

class JSONFormatter(logging.Formatter):
    def format(self, record):
        log_data = {
            'timestamp': datetime.utcnow().isoformat(),
            'level': record.levelname,
            'message': record.getMessage(),
            'module': record.module,
            'function': record.funcName,
            'line': record.lineno
        }

        if hasattr(record, 'request_id'):
            log_data['request_id'] = record.request_id

        if record.exc_info:
            log_data['exception'] = self.formatException(record.exc_info)

        return json.dumps(log_data)

def setup_logging(log_file='llm_service.log'):
    logger = logging.getLogger()
    logger.setLevel(logging.INFO)

    # íŒŒì¼ í•¸ë“¤ëŸ¬ (100MB ë¡œí…Œì´ì…˜)
    file_handler = RotatingFileHandler(
        log_file,
        maxBytes=100*1024*1024,
        backupCount=10
    )
    file_handler.setFormatter(JSONFormatter())

    # ì½˜ì†” í•¸ë“¤ëŸ¬
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    ))

    logger.addHandler(file_handler)
    logger.addHandler(console_handler)

    return logger
```

#### 4. ë¶€í•˜ í…ŒìŠ¤íŠ¸

```python
# load_test.py
import asyncio
import httpx
import time
from statistics import mean, stdev

async def make_request(client, prompt):
    start = time.time()
    try:
        response = await client.post(
            "http://localhost:8080/generate",
            json={
                "prompt": prompt,
                "model": "llama3",
                "backend": "vllm"
            },
            timeout=60.0
        )
        latency = time.time() - start
        return {"success": True, "latency": latency, "status": response.status_code}
    except Exception as e:
        return {"success": False, "latency": time.time() - start, "error": str(e)}

async def load_test(num_requests=100, concurrency=10):
    prompts = [f"Explain concept number {i}" for i in range(num_requests)]

    async with httpx.AsyncClient() as client:
        tasks = []
        for i in range(0, num_requests, concurrency):
            batch = prompts[i:i+concurrency]
            batch_tasks = [make_request(client, p) for p in batch]
            tasks.extend(batch_tasks)

        start_time = time.time()
        results = await asyncio.gather(*tasks)
        total_time = time.time() - start_time

    # í†µê³„ ê³„ì‚°
    successful = [r for r in results if r["success"]]
    failed = [r for r in results if not r["success"]]

    latencies = [r["latency"] for r in successful]

    print(f"\n=== Load Test Results ===")
    print(f"Total requests: {num_requests}")
    print(f"Successful: {len(successful)}")
    print(f"Failed: {len(failed)}")
    print(f"Total time: {total_time:.2f}s")
    print(f"Requests/sec: {num_requests/total_time:.2f}")

    if latencies:
        print(f"\nLatency Statistics:")
        print(f"  Mean: {mean(latencies):.2f}s")
        print(f"  Std Dev: {stdev(latencies):.2f}s")
        print(f"  Min: {min(latencies):.2f}s")
        print(f"  Max: {max(latencies):.2f}s")

if __name__ == "__main__":
    asyncio.run(load_test(num_requests=100, concurrency=10))
```

### ë³´ì•ˆ ë° ì¸ì¦

#### 1. API í‚¤ ì¸ì¦

```python
# auth.py
from fastapi import Security, HTTPException, status
from fastapi.security import APIKeyHeader
from typing import Optional

API_KEY_NAME = "X-API-Key"
api_key_header = APIKeyHeader(name=API_KEY_NAME, auto_error=False)

# í™˜ê²½ ë³€ìˆ˜ë‚˜ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë¡œë“œ
VALID_API_KEYS = {
    "your-secret-key-1": "user1",
    "your-secret-key-2": "user2"
}

async def get_api_key(api_key: str = Security(api_key_header)) -> str:
    if api_key is None:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="API Key missing"
        )

    if api_key not in VALID_API_KEYS:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Invalid API Key"
        )

    return VALID_API_KEYS[api_key]

# app.pyì—ì„œ ì‚¬ìš©
from fastapi import Depends

@app.post("/generate")
async def generate(
    request: GenerateRequest,
    user: str = Depends(get_api_key)
):
    # API í‚¤ ê²€ì¦ í›„ ì²˜ë¦¬
    ...
```

#### 2. Rate Limiting

```python
# rate_limiter.py
from fastapi import HTTPException
import redis
from datetime import datetime, timedelta

redis_client = redis.Redis(host='localhost', port=6379, db=0)

async def check_rate_limit(api_key: str, limit: int = 100, window: int = 3600):
    """ì‹œê°„ë‹¹ ìš”ì²­ ìˆ˜ ì œí•œ"""
    key = f"rate_limit:{api_key}"
    current = redis_client.get(key)

    if current is None:
        redis_client.setex(key, window, 1)
        return

    current = int(current)
    if current >= limit:
        raise HTTPException(
            status_code=429,
            detail=f"Rate limit exceeded. Max {limit} requests per hour."
        )

    redis_client.incr(key)
```

### ìŠ¤ì¼€ì¼ë§ ì „ëµ

#### 1. ìˆ˜í‰ ìŠ¤ì¼€ì¼ë§ (Kubernetes)

```yaml
# k8s-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: vllm-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: vllm
  template:
    metadata:
      labels:
        app: vllm
    spec:
      containers:
      - name: vllm
        image: vllm/vllm-openai:latest
        args:
          - --model
          - meta-llama/Llama-3-8b-hf
          - --tensor-parallel-size
          - "1"
        resources:
          limits:
            nvidia.com/gpu: 1
        ports:
        - containerPort: 8000
---
apiVersion: v1
kind: Service
metadata:
  name: vllm-service
spec:
  selector:
    app: vllm
  ports:
  - port: 8000
    targetPort: 8000
  type: LoadBalancer
```

#### 2. ë¡œë“œ ë°¸ëŸ°ì‹± (HAProxy)

```conf
# haproxy.cfg
global
    maxconn 4096

defaults
    mode http
    timeout connect 5000ms
    timeout client 50000ms
    timeout server 50000ms

frontend vllm_front
    bind *:8000
    default_backend vllm_back

backend vllm_back
    balance roundrobin
    server vllm1 192.168.1.101:8000 check
    server vllm2 192.168.1.102:8000 check
    server vllm3 192.168.1.103:8000 check
```

## í™œìš© ì‚¬ë¡€

### 1. ë¡œì»¬ ì½”ë“œ ì–´ì‹œìŠ¤í„´íŠ¸ (Ollama)

```bash
# CodeLlama ì‹¤í–‰
ollama run codellama

# ì½”ë“œ ë¦¬ë·° ìš”ì²­
>>> Review this Python function:
>>> def factorial(n):
>>>     return 1 if n == 0 else n * factorial(n-1)
```

### 2. ì±„íŒ…ë´‡ ì„œë¹„ìŠ¤ (vLLM)

```python
from fastapi import FastAPI
from openai import OpenAI

app = FastAPI()
client = OpenAI(base_url="http://localhost:8000/v1", api_key="not-needed")

@app.post("/chat")
async def chat(message: str):
    response = client.chat.completions.create(
        model="meta-llama/Llama-3-8b-hf",
        messages=[{"role": "user", "content": message}]
    )
    return {"response": response.choices[0].message.content}
```

### 3. ë¬¸ì„œ ìš”ì•½ íŒŒì´í”„ë¼ì¸ (Ollama)

```python
import requests

def summarize_document(text, model="llama3"):
    response = requests.post(
        "http://localhost:11434/api/generate",
        json={
            "model": model,
            "prompt": f"ë‹¤ìŒ ë¬¸ì„œë¥¼ 3ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”:\n\n{text}",
            "stream": False
        }
    )
    return response.json()["response"]

# ì‚¬ìš©
document = "ê¸´ ë¬¸ì„œ ë‚´ìš©..."
summary = summarize_document(document)
print(summary)
```

### 4. ëŒ€ê·œëª¨ ë°ì´í„° ë¶„ì„ (vLLM)

```python
from vllm import LLM, SamplingParams
import pandas as pd

# ëª¨ë¸ ì´ˆê¸°í™”
llm = LLM(model="meta-llama/Llama-3-8b-hf")

# ëŒ€ëŸ‰ì˜ ê³ ê° ë¦¬ë·° ë¶„ì„
reviews = pd.read_csv("reviews.csv")["review_text"].tolist()

prompts = [
    f"Analyze sentiment (positive/negative/neutral): {review}"
    for review in reviews
]

# ë°°ì¹˜ ì²˜ë¦¬
results = llm.generate(
    prompts,
    SamplingParams(temperature=0.3, max_tokens=10)
)

# ê²°ê³¼ ì €ì¥
sentiments = [output.outputs[0].text.strip() for output in results]
reviews_df["sentiment"] = sentiments
```

### 5. RAG (Retrieval-Augmented Generation) ì‹œìŠ¤í…œ

```python
from vllm import LLM, SamplingParams
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np

# ì„ë² ë”© ëª¨ë¸
embedder = SentenceTransformer('all-MiniLM-L6-v2')

# ì§€ì‹ ë² ì´ìŠ¤
documents = [
    "Pythonì€ 1991ë…„ì— ë°œí‘œëœ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì…ë‹ˆë‹¤.",
    "ë”¥ëŸ¬ë‹ì€ ì¸ê³µ ì‹ ê²½ë§ì„ ì‚¬ìš©í•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë²•ì…ë‹ˆë‹¤.",
    # ... ë” ë§ì€ ë¬¸ì„œ
]

# ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶•
embeddings = embedder.encode(documents)
index = faiss.IndexFlatL2(embeddings.shape[1])
index.add(np.array(embeddings))

# LLM ì´ˆê¸°í™”
llm = LLM(model="meta-llama/Llama-3-8b-hf")

def rag_query(question, k=3):
    # ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
    q_embedding = embedder.encode([question])
    distances, indices = index.search(q_embedding, k)

    context = "\n".join([documents[i] for i in indices[0]])

    # LLMìœ¼ë¡œ ë‹µë³€ ìƒì„±
    prompt = f"""ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”:

Context:
{context}

Question: {question}

Answer:"""

    output = llm.generate(prompt, SamplingParams(temperature=0.7, max_tokens=200))
    return output[0].outputs[0].text

# ì‚¬ìš©
answer = rag_query("Pythonì€ ì–¸ì œ ë§Œë“¤ì–´ì¡Œë‚˜ìš”?")
print(answer)
```

### 6. ë‹¤êµ­ì–´ ë²ˆì—­ ì„œë¹„ìŠ¤ (Ollama)

```python
import requests

class TranslationService:
    def __init__(self, model="llama3"):
        self.model = model
        self.api_url = "http://localhost:11434/api/generate"

    def translate(self, text, source_lang, target_lang):
        prompt = f"""Translate the following text from {source_lang} to {target_lang}.
Only provide the translation, no explanations.

Text: {text}

Translation:"""

        response = requests.post(self.api_url, json={
            "model": self.model,
            "prompt": prompt,
            "stream": False
        })

        return response.json()["response"].strip()

# ì‚¬ìš©
translator = TranslationService()
result = translator.translate(
    "Hello, how are you?",
    "English",
    "Korean"
)
print(result)  # ì•ˆë…•í•˜ì„¸ìš”, ì–´ë–»ê²Œ ì§€ë‚´ì„¸ìš”?
```

---

## ì¶”ê°€ ë¦¬ì†ŒìŠ¤

### Ollama
- ê³µì‹ ì›¹ì‚¬ì´íŠ¸: https://ollama.com
- GitHub: https://github.com/ollama/ollama
- ëª¨ë¸ ë¼ì´ë¸ŒëŸ¬ë¦¬: https://ollama.ai/library

### vLLM
- ê³µì‹ ë¬¸ì„œ: https://docs.vllm.ai
- GitHub: https://github.com/vllm-project/vllm
- ë…¼ë¬¸: [Efficient Memory Management for Large Language Model Serving with PagedAttention](https://arxiv.org/abs/2309.06180)

### ëª¨ë¸ í—ˆë¸Œ
- Hugging Face: https://huggingface.co/models
- Ollama Library: https://ollama.com/library

